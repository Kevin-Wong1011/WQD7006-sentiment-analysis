{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Analysis with TF-IDF text representation\n",
        "\n",
        "## Our Dataset\n",
        "\n",
        "This dataset describes the contents of the heart-disease diagnosis.\n",
        "\n",
        "The dataset in this study is from [Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment/data), which is called Twitter US Airline Sentiment.\n",
        "\n",
        "- Dataset: https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment/data\n",
        "\n",
        "## Variable Table\n",
        "\n",
        "| Original Dataset             | Data Type     | Description    |                                                         \n",
        "|------------------------------|---------------|---------------------------------------------------------------------------------------------|\n",
        "| tweet_id                     | ID            | A unique identifier for each tweet.                                                         | \n",
        "| airline_sentiment            | Categorical   | The sentiment expressed in the tweet (positive, neutral, negative).                         | \n",
        "| airline_sentiment_confidence | Numerical     | Confidence score in the sentiment label (0 to 1).                                           | \n",
        "| negativereason               | Categorical   | Reason for negative sentiment (e.g., \"Late Flight\", \"Customer Service Issue\").              | \n",
        "| negativereason_confidence    | Numerical     | Confidence score in the negative reason label (0 to 1).                                     | \n",
        "| airline                      | Categorical   | The airline mentioned in the tweet (e.g., United, Delta, etc.).                             | \n",
        "| airline_sentiment_gold       | Categorical   | Sentiment label by trusted annotator (gold standard).                                       | \n",
        "| name                         | Text          | Name of the user who posted the tweet.                                                      | \n",
        "| negativereason_gold          | Categorical   | Negative reason label by trusted annotator (gold standard).                                 | \n",
        "| retweet_count                | Numerical     | Number of times the tweet was retweeted.                                                    | \n",
        "| text                         | Text          | The full content of the tweet.                                                              | \n",
        "| tweet_coord                  | Geospatial    | Latitude and longitude coordinates where the tweet was posted, if available.                | \n",
        "| tweet_created                | Datetime      | Timestamp when the tweet was created.                                                       | \n",
        "| tweet_location               | Text          | Location specified in the user's profile.                                                   | \n",
        "| user_timezone                | Categorical   | Time zone specified in the user's profile.                                                  | \n",
        "\n",
        "<br/>\n",
        "\n",
        "## Data Used for Modeling\n",
        "\n",
        "| Feature                      | Data Type   | Description  |\n",
        "|-----------------------------|-------------|--------------|\n",
        "| **Target Variable: `encoded_sentiment`** | Categorical | This is an engineered variable derived from `airline_sentiment` for multi-class sentiment classification. It encodes sentiment as: 0 = Negative, 1 = Neutral, 2 = Positive. |\n",
        "| **Feature: `text`**         | Text        | Contains consumer tweets about U.S. airlines. This field undergoes preprocessing, including removal of URLs and mentions (`@`), stopword removal, and stemming. |\n",
        "\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOda8zvDG_Dr"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c48xxsXVYTVX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"../../data/tweets.csv.gz\", compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14640 entries, 0 to 14639\n",
            "Data columns (total 15 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   tweet_id                      14640 non-null  int64  \n",
            " 1   airline_sentiment             14640 non-null  object \n",
            " 2   airline_sentiment_confidence  14640 non-null  float64\n",
            " 3   negativereason                9178 non-null   object \n",
            " 4   negativereason_confidence     10522 non-null  float64\n",
            " 5   airline                       14640 non-null  object \n",
            " 6   airline_sentiment_gold        40 non-null     object \n",
            " 7   name                          14640 non-null  object \n",
            " 8   negativereason_gold           32 non-null     object \n",
            " 9   retweet_count                 14640 non-null  int64  \n",
            " 10  text                          14640 non-null  object \n",
            " 11  tweet_coord                   1019 non-null   object \n",
            " 12  tweet_created                 14640 non-null  object \n",
            " 13  tweet_location                9907 non-null   object \n",
            " 14  user_timezone                 9820 non-null   object \n",
            "dtypes: float64(2), int64(2), object(11)\n",
            "memory usage: 1.7+ MB\n"
          ]
        }
      ],
      "source": [
        "# Show basic info\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "506zNhdXYOyS",
        "outputId": "dc47c80f-905b-40b1-b2ef-ff52f5be0dbd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0  570306133677760513           neutral                        1.0000   \n",
              "1  570301130888122368          positive                        0.3486   \n",
              "2  570301083672813571           neutral                        0.6837   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0            NaN                        NaN  Virgin America   \n",
              "1            NaN                        0.0  Virgin America   \n",
              "2            NaN                        NaN  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
              "0                    NaN     cairdin                 NaN              0   \n",
              "1                    NaN    jnardino                 NaN              0   \n",
              "2                    NaN  yvonnalynn                 NaN              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.         NaN   \n",
              "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
              "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
              "\n",
              "               tweet_created tweet_location               user_timezone  \n",
              "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
              "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
              "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the first few rows\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "162TfnxmYSgM"
      },
      "source": [
        "# 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR7oZ2rA6RGf"
      },
      "source": [
        "## 2.1 Handle Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmzCb0Cd6RTH",
        "outputId": "de3cf7ee-adfd-4a62-8179-1a384b075664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 36\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "print(f\"Number of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Drop duplicate rows\n",
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCR61Dbt6TG9",
        "outputId": "636ce20f-e747-4164-98b9-4dad4c14d8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after dropping duplicates: (14604, 15)\n"
          ]
        }
      ],
      "source": [
        "# Confirm the shape after removal\n",
        "print(f\"Shape after dropping duplicates: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E2TL3cPYQ_b"
      },
      "source": [
        "## 2.2 Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QKZcizXmYOC",
        "outputId": "7e8b49c5-498a-4ec8-ee97-771df3186ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values count for each variables:\n",
            "-------------------------------------------\n",
            "tweet_id                            0\n",
            "airline_sentiment                   0\n",
            "airline_sentiment_confidence        0\n",
            "negativereason                   5445\n",
            "negativereason_confidence        4101\n",
            "airline                             0\n",
            "airline_sentiment_gold          14564\n",
            "name                                0\n",
            "negativereason_gold             14572\n",
            "retweet_count                       0\n",
            "text                                0\n",
            "tweet_coord                     13589\n",
            "tweet_created                       0\n",
            "tweet_location                   4723\n",
            "user_timezone                    4814\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "**Note**: We won't remove any rows with missing values here as \n",
            "our main field we use is 'text' and 'airline_sentiment' column,\n",
            "which has no missing values\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values for each variables in the dataset\n",
        "print(\"\\nMissing values count for each variables:\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\"\"\\n\\n**Note**: We won't remove any rows with missing values here as \n",
        "our main field we use is 'text' and 'airline_sentiment' column,\n",
        "which has no missing values\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYy8fCwcZUH5"
      },
      "source": [
        "## 2.3 Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\tys\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\tys\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\tys\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3.1 Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For feature engineering in sentiment analysis, we will perform the following steps:\n",
        "\n",
        "- `Tokenization`: Breaking the text into individual words or tokens.\n",
        "- `Stopwords Removal`: Eliminating common words (e.g., \"the\", \"is\", \"and\") that don't contribute meaningful information.\n",
        "- `Stemming`: Reducing words to their base or root form (e.g., \"running\" → \"run\", \"happily\" → \"happy\").\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "axOWklH0wfRE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Initialize stopwords, stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Step 1: Lowercase and clean the text\n",
        "def clean_text(text):\n",
        "    text = text.lower()                                 # Lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text) # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)                    # Remove mentions\n",
        "    # text = re.sub(r'#\\w+', '', text)                    # NOTE: Do not remove hashtags, \n",
        "                                                                # as there is a lot of hashtags with sentiment indication, \n",
        "                                                                # such as '#thankyou', '#happycustomer', etc...\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)                # Remove numbers and punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Step 2: Tokenization\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Step 3: Remove stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Step 4: Apply Stemming\n",
        "def apply_stemming(tokens):\n",
        "    return \" \".join([stemmer.stem(word) for word in tokens])\n",
        "\n",
        "# Copy only the 'text' column to df_copy\n",
        "df2 = df[['text', 'airline_sentiment']].copy()\n",
        "\n",
        "# rule-based text processing for removing URL, twitter username and punctutations\n",
        "df2['clean_text'] = df2['text'].apply(clean_text)\n",
        "\n",
        "# procesed text after stemming\n",
        "df2['tokens'] = df2['clean_text'].apply(tokenize_text)\n",
        "df2['removed_stopwords'] = df2['tokens'].apply(remove_stopwords)\n",
        "df2['stemmed'] = df2['removed_stopwords'].apply(apply_stemming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14604, 6)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values per column:\n",
            "text                 0\n",
            "airline_sentiment    0\n",
            "clean_text           0\n",
            "tokens               0\n",
            "removed_stopwords    0\n",
            "stemmed              0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values per column\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df2.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyXO_XyyWhaO"
      },
      "source": [
        "### 2.3.2 Target Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will convert the 'airline_sentiment' column into numerical values to use it as the target variable in our model, where `negative` = 0, `neutral` = 1, and `positive` = 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QgxJ-qbEVYOY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The target variable contains unique values of:  ['neutral' 'positive' 'negative'] which we are going to map it into 0, 1 and 2 respectively\n"
          ]
        }
      ],
      "source": [
        "print(\"The target variable contains unique values of: \", df2['airline_sentiment'].unique(), \n",
        "      \"which we are going to map it into 0, 1 and 2 respectively\")\n",
        "\n",
        "# Encode the sentiment column\n",
        "df2['encoded_sentiment'] = df2['airline_sentiment'].map({\"negative\": 0, \"neutral\": 1, \"positive\": 2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.0 Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into features (X) and target labels (y)\n",
        "X = df2['stemmed']\n",
        "y = df2['encoded_sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Train test valid split with stratified sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1sEyd2cK2JFr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train test validation split with ration 70:15:15\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, \n",
        "                                        stratify=y,            \n",
        "                                        train_size=0.7, \n",
        "                                        random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, \n",
        "                                        stratify=y_temp,            \n",
        "                                        train_size=0.5, \n",
        "                                        random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train class distribution:\n",
            "encoded_sentiment\n",
            "0    0.627177\n",
            "1    0.211602\n",
            "2    0.161221\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Validation class distribution:\n",
            "encoded_sentiment\n",
            "0    0.627111\n",
            "1    0.211775\n",
            "2    0.161114\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Test class distribution:\n",
            "encoded_sentiment\n",
            "0    0.627111\n",
            "1    0.211775\n",
            "2    0.161114\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "**Observation**: The class proportions appear to be preserved across the training and test sets, \n",
            "indicating a successful stratified split.\n"
          ]
        }
      ],
      "source": [
        "# Verify that class distribution is preserved after the train-test split (i.e., stratified correctly)\n",
        "\n",
        "print(\"Train class distribution:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nValidation class distribution:\")\n",
        "print(y_valid.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nTest class distribution:\")\n",
        "print(y_test.value_counts(normalize=True))\n",
        "\n",
        "print(\"\"\"\\n**Observation**: The class proportions appear to be preserved across the training and test sets, \n",
        "indicating a successful stratified split.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Text Representation with Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word2Vec represents words as dense vectors in a continuous vector space, capturing semantic and syntactic relationships between words based on their context. Instead of counting how often a word appears, Word2Vec learns how words are used in similar contexts across a corpus. Words that appear in similar contexts will have similar vector representations, allowing the model to understand relationships like analogies or word similarities.\n",
        "\n",
        "In this notebook, we will be using word2vec embedding with dimension size of 300."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec \n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# File paths\n",
        "train_path = \"../../data/word2vec_embedding_train.csv.gz\"\n",
        "valid_path = \"../../data/word2vec_embedding_valid.csv.gz\"\n",
        "test_path = \"../../data/word2vec_embedding_test.csv.gz\"\n",
        "\n",
        "# Check if files exist\n",
        "if os.path.exists(train_path) and os.path.exists(valid_path):\n",
        "    # Load precomputed embeddings\n",
        "    X_train_vectorized = pd.read_csv(train_path, compression=\"gzip\", index_col=False).values\n",
        "    X_valid_vectorized = pd.read_csv(valid_path, compression=\"gzip\", index_col=False).values\n",
        "    \n",
        "else:\n",
        "    # Tokenize cleaned strings for Word2Vec\n",
        "    tokenized_train = [word_tokenize(text) for text in X_train]\n",
        "    tokenized_valid = [word_tokenize(text) for text in X_valid]\n",
        "    tokenized_test = [word_tokenize(text) for text in X_test]\n",
        "\n",
        "    # Train Word2Vec on all tokens\n",
        "    w2v_model = Word2Vec(\n",
        "        sentences=tokenized_train + tokenized_valid + tokenized_test, \n",
        "        vector_size=300, \n",
        "        window=5, \n",
        "        min_count=1, \n",
        "        workers=4\n",
        "    )\n",
        "\n",
        "    # Average vector generator\n",
        "    def get_avg_w2v(tokens, model, k=300):\n",
        "        vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
        "        return np.mean(vecs, axis=0) if vecs else np.zeros(k)\n",
        "\n",
        "    # Create average embeddings\n",
        "    X_train_vectorized = np.vstack([get_avg_w2v(t, w2v_model) for t in tokenized_train])\n",
        "    X_valid_vectorized = np.vstack([get_avg_w2v(t, w2v_model) for t in tokenized_valid])\n",
        "    X_test_vectorized = np.vstack([get_avg_w2v(t, w2v_model) for t in tokenized_test])\n",
        "\n",
        "    # Save to CSV\n",
        "    pd.DataFrame(X_train_vectorized).to_csv(train_path, index=False)\n",
        "    pd.DataFrame(X_valid_vectorized).to_csv(valid_path, index=False)\n",
        "    pd.DataFrame(X_test_vectorized).to_csv(test_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Handling class imbalance issue with SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4UxzlN66l61P"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Handling imbalanced using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC_xi5cMoIi8",
        "outputId": "39644d0c-12a5-4a81-8023-53af08425342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution in training set (before SMOTE):\n",
            "encoded_sentiment\n",
            "0    6411\n",
            "1    2163\n",
            "2    1648\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution in training set (after SMOTE):\n",
            "encoded_sentiment\n",
            "1    6411\n",
            "0    6411\n",
            "2    6411\n",
            "Name: count, dtype: int64\n",
            "\n",
            "**Observation**: The class distribution in the training set has been balanced after applying SMOTE, \n",
            "confirming that oversampling was successful.\n"
          ]
        }
      ],
      "source": [
        "# Check class distribution before and after applying SMOTE to confirm successful balancing\n",
        "\n",
        "print(\"Class distribution in training set (before SMOTE):\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\nClass distribution in training set (after SMOTE):\")\n",
        "print(y_train_resampled.value_counts())\n",
        "\n",
        "print(\"\"\"\\n**Observation**: The class distribution in the training set has been balanced after applying SMOTE, \n",
        "confirming that oversampling was successful.\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2diDHuHrrwV"
      },
      "source": [
        "## 4.0 Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(i) Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.6211\n",
            "Validation Accuracy:  0.6047\n",
            "Test Accuracy:  0.5888\n",
            "\n",
            "Train F1-score: 0.6221\n",
            "Validation F1-score:  0.6243\n",
            "Test F1-score:  0.6094\n",
            "\n",
            "Train Precision: 0.6335\n",
            "Validation Precision:  0.7003\n",
            "Test F1-score:  0.6814\n",
            "\n",
            "Train Recall: 0.6211\n",
            "Validation Recall:  0.6047\n",
            "Test Recall:  0.5888\n",
            "\n",
            "Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.58      0.69      1374\n",
            "           1       0.37      0.67      0.48       464\n",
            "           2       0.45      0.53      0.49       353\n",
            "\n",
            "    accuracy                           0.59      2191\n",
            "   macro avg       0.56      0.59      0.55      2191\n",
            "weighted avg       0.68      0.59      0.61      2191\n",
            "\n",
            "Confusion Matrix (Test):\n",
            "[[791 421 162]\n",
            " [ 83 311  70]\n",
            " [ 61 104 188]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (f1_score, accuracy_score, precision_score, \n",
        "                             recall_score, classification_report, confusion_matrix)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predictions on training and valid sets\n",
        "y_train_pred_lr = lr_model.predict(X_train_resampled)\n",
        "y_valid_pred_lr = lr_model.predict(X_valid_vectorized)\n",
        "y_test_pred_lr = lr_model.predict(X_test_vectorized)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train_resampled, y_train_pred_lr)\n",
        "valid_accuracy = accuracy_score(y_valid, y_valid_pred_lr)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred_lr)\n",
        "\n",
        "# F1 scores\n",
        "train_f1_score = f1_score(y_train_resampled, y_train_pred_lr, average='weighted')\n",
        "valid_f1_score = f1_score(y_valid, y_valid_pred_lr, average='weighted')\n",
        "test_f1_score = f1_score(y_test, y_test_pred_lr, average='weighted')\n",
        "\n",
        "# Precision scores\n",
        "train_precision = precision_score(y_train_resampled, y_train_pred_lr, average='weighted')\n",
        "valid_precision = precision_score(y_valid, y_valid_pred_lr, average='weighted')\n",
        "test_precision = precision_score(y_test, y_test_pred_lr, average='weighted')\n",
        "\n",
        "# Recall scores\n",
        "train_recall = recall_score(y_train_resampled, y_train_pred_lr, average='weighted')\n",
        "valid_recall = recall_score(y_valid, y_valid_pred_lr, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred_lr, average='weighted')\n",
        "\n",
        "\n",
        "# Output\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Validation Accuracy:  {valid_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy:  {test_accuracy:.4f}\\n\")\n",
        "\n",
        "print(f\"Train F1-score: {train_f1_score:.4f}\")\n",
        "print(f\"Validation F1-score:  {valid_f1_score:.4f}\")\n",
        "print(f\"Test F1-score:  {test_f1_score:.4f}\\n\")\n",
        "\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Validation Precision:  {valid_precision:.4f}\")\n",
        "print(f\"Test F1-score:  {test_precision:.4f}\\n\")\n",
        "\n",
        "\n",
        "print(f\"Train Recall: {train_recall:.4f}\")\n",
        "print(f\"Validation Recall:  {valid_recall:.4f}\")\n",
        "print(f\"Test Recall:  {test_recall:.4f}\\n\")\n",
        "\n",
        "print(\"Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred_lr))\n",
        "\n",
        "print(\"Confusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_test_pred_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(ii) Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyZAwecKrwN4",
        "outputId": "54ba3649-b3c6-41d3-de2f-93ad5596c9c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.6512\n",
            "Valid Accuracy:  0.5678\n",
            "Test Accuracy:  0.5582\n",
            "\n",
            "Train F1-score: 0.6510\n",
            "Valid F1-score:  0.5907\n",
            "Test F1-score:  0.5806\n",
            "\n",
            "Train Precision: 0.6527\n",
            "Valid Precision:  0.6632\n",
            "Valid Precision:  0.6632\n",
            "\n",
            "Train Recall: 0.6512\n",
            "Valid Recall:  0.5678\n",
            "Test Recall:  0.5582\n",
            "\n",
            "Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.57      0.67      1374\n",
            "           1       0.36      0.56      0.44       464\n",
            "           2       0.36      0.53      0.43       353\n",
            "\n",
            "    accuracy                           0.56      2191\n",
            "   macro avg       0.51      0.55      0.51      2191\n",
            "weighted avg       0.65      0.56      0.58      2191\n",
            "\n",
            "Confusion Matrix (Test):\n",
            "[[777 354 243]\n",
            " [114 258  92]\n",
            " [ 62 103 188]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (f1_score, accuracy_score, precision_score, \n",
        "                             recall_score, classification_report, confusion_matrix)\n",
        "\n",
        "# Initialize the Decision Tree\n",
        "dt_model = DecisionTreeClassifier(max_depth=100, \n",
        "                                  min_samples_split=10, \n",
        "                                  criterion='entropy', \n",
        "                                  min_samples_leaf=100,\n",
        "                                  random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predictions on training and test sets\n",
        "y_train_pred_dt = dt_model.predict(X_train_resampled)\n",
        "y_valid_pred_dt = dt_model.predict(X_valid_vectorized)\n",
        "y_test_pred_dt = dt_model.predict(X_test_vectorized)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train_resampled, y_train_pred_dt)\n",
        "valid_accuracy = accuracy_score(y_valid, y_valid_pred_dt)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred_dt)\n",
        "\n",
        "# F1 scores\n",
        "train_f1_score = f1_score(y_train_resampled, y_train_pred_dt, average='weighted')\n",
        "valid_f1_score = f1_score(y_valid, y_valid_pred_dt, average='weighted')\n",
        "test_f1_score = f1_score(y_test, y_test_pred_dt, average='weighted')\n",
        "\n",
        "# Precision scores\n",
        "train_precision = precision_score(y_train_resampled, y_train_pred_dt, average='weighted')\n",
        "valid_precision = precision_score(y_valid, y_valid_pred_dt, average='weighted')\n",
        "test_precision = precision_score(y_test, y_test_pred_dt, average='weighted')\n",
        "\n",
        "# Recall scores\n",
        "train_recall = recall_score(y_train_resampled, y_train_pred_dt, average='weighted')\n",
        "valid_recall = recall_score(y_valid, y_valid_pred_dt, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred_dt, average='weighted')\n",
        "\n",
        "# Output\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Valid Accuracy:  {valid_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy:  {test_accuracy:.4f}\\n\")\n",
        "\n",
        "print(f\"Train F1-score: {train_f1_score:.4f}\")\n",
        "print(f\"Valid F1-score:  {valid_f1_score:.4f}\")\n",
        "print(f\"Test F1-score:  {test_f1_score:.4f}\\n\")\n",
        "\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Valid Precision:  {valid_precision:.4f}\")\n",
        "print(f\"Valid Precision:  {valid_precision:.4f}\\n\")\n",
        "\n",
        "print(f\"Train Recall: {train_recall:.4f}\")\n",
        "print(f\"Valid Recall:  {valid_recall:.4f}\")\n",
        "print(f\"Test Recall:  {test_recall:.4f}\\n\")\n",
        "\n",
        "print(\"Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred_dt))\n",
        "\n",
        "print(\"Confusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_test_pred_dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(iii) XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved.\n",
            "Train Accuracy: 0.9946\n",
            "Valid Accuracy:  0.6764\n",
            "Test Accuracy:  0.6737\n",
            "\n",
            "Train F1-score: 0.9946\n",
            "Valid F1-score:  0.6781\n",
            "Test F1-score:  0.6729\n",
            "\n",
            "Train Precision: 0.9947\n",
            "Valid Precision:  0.6801\n",
            "Test Precision:  0.6723\n",
            "\n",
            "Train Recall: 0.9946\n",
            "Valid Recall:  0.6764\n",
            "Test Recall:  0.6737\n",
            "\n",
            "Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.79      0.78      1374\n",
            "           1       0.46      0.46      0.46       464\n",
            "           2       0.53      0.51      0.52       353\n",
            "\n",
            "    accuracy                           0.67      2191\n",
            "   macro avg       0.59      0.59      0.59      2191\n",
            "weighted avg       0.67      0.67      0.67      2191\n",
            "\n",
            "Confusion Matrix (Test):\n",
            "[[1082  186  106]\n",
            " [ 195  214   55]\n",
            " [ 109   64  180]]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import (f1_score, accuracy_score, precision_score, \n",
        "                             recall_score, classification_report, confusion_matrix)\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "model_path = \"../../models/word2vec_embedding_xgb_model.pkl\"\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading existing model...\")\n",
        "    xgb_model = joblib.load(model_path)\n",
        "else:\n",
        "    # Train the model\n",
        "    xgb_model = xgb.XGBClassifier(max_depth=10,\n",
        "                                random_state=42,\n",
        "                                # Introduce randomness to make training faster and reduce overfitting\n",
        "                                subsample=0.8, ## Uses 80% of the data for each tree.\n",
        "                                colsample_bytree=0.8, ## Uses 80% of the features for each tree.\n",
        "                                # the parameters below make the model trained faster by enabling parallelism\n",
        "                                n_jobs = -1)\n",
        "    xgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "    # Export and load previously trained model to avoid retraining every time\n",
        "    joblib.dump(xgb_model, model_path)\n",
        "    print(\"Model saved.\")\n",
        "\n",
        "# Predictions on training and test sets\n",
        "y_train_pred_xgb = xgb_model.predict(X_train_resampled)\n",
        "y_valid_pred_xgb = xgb_model.predict(X_valid_vectorized)\n",
        "y_test_pred_xgb = xgb_model.predict(X_test_vectorized)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train_resampled, y_train_pred_xgb)\n",
        "valid_accuracy = accuracy_score(y_valid, y_valid_pred_xgb)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred_xgb)\n",
        "\n",
        "# F1 scores\n",
        "train_f1_score = f1_score(y_train_resampled, y_train_pred_xgb, average='weighted')\n",
        "valid_f1_score = f1_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
        "test_f1_score = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
        "\n",
        "# Precision scores\n",
        "train_precision = precision_score(y_train_resampled, y_train_pred_xgb, average='weighted')\n",
        "valid_precision = precision_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
        "test_precision = precision_score(y_test, y_test_pred_xgb, average='weighted')\n",
        "\n",
        "# Recall scores\n",
        "train_recall = recall_score(y_train_resampled, y_train_pred_xgb, average='weighted')\n",
        "valid_recall = recall_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred_xgb, average='weighted')\n",
        "\n",
        "# Output\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Valid Accuracy:  {valid_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy:  {test_accuracy:.4f}\\n\")\n",
        "\n",
        "print(f\"Train F1-score: {train_f1_score:.4f}\")\n",
        "print(f\"Valid F1-score:  {valid_f1_score:.4f}\")\n",
        "print(f\"Test F1-score:  {test_f1_score:.4f}\\n\")\n",
        "\n",
        "print(f\"Train Precision: {train_precision:.4f}\")\n",
        "print(f\"Valid Precision:  {valid_precision:.4f}\")\n",
        "print(f\"Test Precision:  {test_precision:.4f}\\n\")\n",
        "\n",
        "print(f\"Train Recall: {train_recall:.4f}\")\n",
        "print(f\"Valid Recall:  {valid_recall:.4f}\")\n",
        "print(f\"Test Recall:  {test_recall:.4f}\\n\")\n",
        "\n",
        "print(\"Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred_xgb))\n",
        "\n",
        "print(\"Confusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_test_pred_xgb))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
