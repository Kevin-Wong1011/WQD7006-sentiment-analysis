{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f173c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load and Prepare the Data ---\n",
    "df = pd.read_csv('../../data/tweets.csv.gz', compression=\"gzip\")\n",
    "# Keep only the relevant columns and drop rows with missing text\n",
    "df = df[['airline_sentiment', 'text']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4133c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform multiclass classification (positive, negative, neutral)\n",
    "# Map the sentiment labels to numerical values for the model\n",
    "df[\"airline_sentiment_encoded\"] = df[\"airline_sentiment\"].map({\"negative\": 0, \"neutral\": 1, \"positive\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110d86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features (X) and target variable (y)\n",
    "X = df['text']\n",
    "y = df['airline_sentiment_encoded'] # Use the encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30d47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Split Data for Training and Testing ---\n",
    "# Split the data into training and testing sets.\n",
    "# Using stratify=y ensures the proportion of each sentiment is the same in both sets.\n",
    "# The random_state parameter ensures that the data is split in the same way every time.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split before SMOTE\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, \n",
    "                                        stratify=y,            \n",
    "                                        train_size=0.7, \n",
    "                                        random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, \n",
    "                                        stratify=y_temp,            \n",
    "                                        train_size=0.5, \n",
    "                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b851fb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524127544fde43aeb666f4043ac44b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb23becafffe4adf9a249ac5c254e227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fee977eacf484db1cdee3b1e73f976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 3. Generate SBERT Embeddings for train test validation split ---\n",
    "# We will use a pre-trained SBERT model to convert the tweet text into numerical vectors.\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_TRAIN = \"../../data/sbert_embedding_train.csv.gz\"\n",
    "\n",
    "## Load sbert embedding for train set\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X_train_sbert_embedding = sbert_model.encode(X_train.tolist(), show_progress_bar=True)\n",
    "\n",
    "## Load sbert embedding for validation set\n",
    "X_valid_sbert_embedding = sbert_model.encode(X_valid.tolist(), show_progress_bar=True)    \n",
    "\n",
    "## Load sbert embedding for test validation set\n",
    "X_test_sbert_embedding = sbert_model.encode(X_test.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0d4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3. Handling class imbalance issue with SMOTE --- \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Handling imbalanced using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_sbert_embedding, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2a3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19275, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resampled.shape\n",
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb536a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Hyperparameter Tuning with GridSearchCV ---\n",
    "print(\"\\nStarting Hyperparameter Tuning...\")\n",
    "\n",
    "# Define the XGBoost classifier.\n",
    "# We set a random_state for reproducibility of the model's internal processes.\n",
    "# The 'objective' is set to 'multi:softprob' for multi-class classification.\n",
    "# We also set 'use_label_encoder=False' to avoid a deprecation warning.\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42 # Set random_state for reproducibility\n",
    ")\n",
    "\n",
    "xgb_clf = XGBClassifier(max_depth=10,\n",
    "                            random_state=42,\n",
    "                            # Introduce randomness to make training faster and reduce overfitting\n",
    "                            subsample=0.8, ## Uses 80% of the data for each tree.\n",
    "                            colsample_bytree=0.8, ## Uses 80% of the features for each tree.\n",
    "                            # the parameters below make the model trained faster by enabling parallelism\n",
    "                            n_jobs = -1)\n",
    "\n",
    "# Define the parameter grid to search.\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV to find the best parameters.\n",
    "# cv=3 specifies 3-fold cross-validation.\n",
    "# n_jobs=-1 will use all available CPU cores to speed up the process.\n",
    "# verbose=2 will print progress updates.\n",
    "# Note: The cross-validation splitting in GridSearchCV is also a random process.\n",
    "# By default, it uses StratifiedKFold for classifiers, which has a shuffle=True default.\n",
    "# While not strictly necessary if train_test_split is seeded, it's good practice to also control this.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    # cv=cv_strategy, # Use the defined cross-validation strategy\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data. This will train and evaluate the model with all parameter combinations.\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78cca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. Evaluate the Best Model ---\n",
    "print(\"\\nHyperparameter tuning complete.\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model found by GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbea16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "# For 'multi:softprob', predict_proba() gives probabilities for each class.\n",
    "# We use np.argmax to get the class with the highest probability.\n",
    "\n",
    "### (A) Evaluate the model's performance for Valid set\n",
    "y_valid_proba = best_model.predict_proba(X_valid)\n",
    "y_valid = np.argmax(y_valid_proba, axis=1)\n",
    "\n",
    "print(\"\\n--- Final Model Performance on Test Set ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_valid):.4f}\")\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_test, y_valid))\n",
    "\n",
    "### (B) Evaluate the model's performance for Test set\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(\"\\n--- Final Model Performance on Test Set ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44278d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
